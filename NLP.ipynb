{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/elainny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elainny/anaconda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Word cloud plots for Names\n",
    "from wordcloud import WordCloud,ImageColorGenerator\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "# LDA & LSI packages\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import similarities\n",
    "# Random forest packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tfid\n",
    "from sklearn.naive_bayes import MultinomialNB as multi\n",
    "from sklearn.ensemble import RandomForestClassifier as randomf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report as report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datasets\n",
    "df = pd.read_csv('Data/cleaned-data.csv')\n",
    "train = pd.read_csv('Data/cleaned-train.csv')\n",
    "test = pd.read_csv('Data/cleaned-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>Description</th>\n",
       "      <th>Name</th>\n",
       "      <th>DataType</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Nibble is a 3+ month old ball of cuteness. He ...</td>\n",
       "      <td>Nibble</td>\n",
       "      <td>train</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>I just found it alone yesterday near my apartm...</td>\n",
       "      <td>No Name Yet</td>\n",
       "      <td>train</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Their pregnant mother was dumped by her irresp...</td>\n",
       "      <td>Brisco</td>\n",
       "      <td>train</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Good guard dog, very alert, active, obedience ...</td>\n",
       "      <td>Miko</td>\n",
       "      <td>train</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>This handsome yet cute boy is up for adoption....</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>train</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>This is a stray kitten that came to my house. ...</td>\n",
       "      <td>No name</td>\n",
       "      <td>train</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>anyone within the area of ipoh or taiping who ...</td>\n",
       "      <td>BULAT</td>\n",
       "      <td>train</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Siu Pak just give birth on 13/6/10 to 6puppies...</td>\n",
       "      <td>Siu Pak &amp; Her 6 Puppies</td>\n",
       "      <td>train</td>\n",
       "      <td>Dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>healthy and active, feisty kitten found in nei...</td>\n",
       "      <td>No name</td>\n",
       "      <td>train</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Very manja and gentle stray cat found, we woul...</td>\n",
       "      <td>Kitty</td>\n",
       "      <td>train</td>\n",
       "      <td>Cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   AdoptionSpeed                                        Description  \\\n",
       "0            2.0  Nibble is a 3+ month old ball of cuteness. He ...   \n",
       "1            0.0  I just found it alone yesterday near my apartm...   \n",
       "2            3.0  Their pregnant mother was dumped by her irresp...   \n",
       "3            2.0  Good guard dog, very alert, active, obedience ...   \n",
       "4            2.0  This handsome yet cute boy is up for adoption....   \n",
       "5            2.0  This is a stray kitten that came to my house. ...   \n",
       "6            1.0  anyone within the area of ipoh or taiping who ...   \n",
       "7            3.0  Siu Pak just give birth on 13/6/10 to 6puppies...   \n",
       "8            1.0  healthy and active, feisty kitten found in nei...   \n",
       "9            4.0  Very manja and gentle stray cat found, we woul...   \n",
       "\n",
       "                      Name DataType Type  \n",
       "0                   Nibble    train  Cat  \n",
       "1              No Name Yet    train  Cat  \n",
       "2                   Brisco    train  Dog  \n",
       "3                     Miko    train  Dog  \n",
       "4                   Hunter    train  Dog  \n",
       "5                  No name    train  Cat  \n",
       "6                    BULAT    train  Cat  \n",
       "7  Siu Pak & Her 6 Puppies    train  Dog  \n",
       "8                  No name    train  Cat  \n",
       "9                    Kitty    train  Cat  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select useful columns for NLP\n",
    "df_nlp = df[['AdoptionSpeed','Description','Name','DataType','Type']]\n",
    "df_nlp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# word cloud: cat name\n",
    "plt.figure(figsize=(20, 8))\n",
    "bg_pic = imread('dog-paw.png')\n",
    "image_colors = ImageColorGenerator(bg_pic)\n",
    "\n",
    "\n",
    "cat_name = ' '.join(df.loc[df['Type'] == 'Cat', 'Name'].fillna('').values)\n",
    "wc_cat = WordCloud(mask=bg_pic,background_color='white',scale=20,max_words=300).generate(cat_name)\n",
    "\n",
    "plt.imshow(wc_cat.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('cat.png')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# wordÂ cloud: dog name\n",
    "plt.figure(figsize=(20, 8))\n",
    "dog_name = ' '.join(df.loc[df['Type'] == 'Dog', 'Name'].fillna('').values)\n",
    "wc_dog = WordCloud(mask=bg_pic,background_color='white',scale=20,max_words=300).generate(dog_name)\n",
    "plt.imshow(wc_dog.recolor(color_func=image_colors), interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('dog.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA & LSI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train_data by selecting Description columns in train data\n",
    "train_data = train['Description'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwords, lemmatizer, and data cleaning process\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "#Lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def cleandata(review) :\n",
    "    clean_des = re.sub('[^a-zA-Z]', ' ', str(review)) # Remove punctuation/words not starting with alphabet\n",
    "    clean_des = clean_des.lower() # make words lower cases\n",
    "    words = word_tokenize(clean_des) # tokenize\n",
    "    words = [w for w in words if not w in stop_words] # stop words removal\n",
    "    words = [wordnet_lemmatizer.lemmatize(w) for w in words] #Lemmatize words\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics\n",
    "t = 10\n",
    "\n",
    "cleaned = []\n",
    "for description in train_data:\n",
    "    cleaned.append(cleandata(description))\n",
    "# Create a Dictionary associate word to id\n",
    "D = corpora.Dictionary(cleaned)\n",
    "\n",
    "# Transform texts to numeric\n",
    "corpra = [D.doc2bow(i) for i in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA model\n",
    "lda = models.LdaModel(corpus=corpra, num_topics=t, id2word=D)\n",
    "\n",
    "print('LDA model')\n",
    "for index in range(0,t):\n",
    "    # top 9 topics\n",
    "    print(\"Topic Number %s:\" % str(index+1), lda.print_topic(index, 9))\n",
    "print(\"-\" * 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LSI model\n",
    "lsi = models.LsiModel(corpus=corpra, num_topics=t, id2word=D)\n",
    "\n",
    "print('LSI model')\n",
    "for index in range(0,t):\n",
    "    # top 9 topics\n",
    "    print(\"Topic Number %s:\" % str(index+1), lsi.print_topic(index, 9))\n",
    "print(\"-\" * 117)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick one description from test to predict similarity.\n",
    "import random\n",
    "i = random.randint(1,3948) # since my test dataset has 3498 values \n",
    "print(i)\n",
    "test_data = test.loc[i,'Description']\n",
    "print('-----This is the description from test that I am going to predict:-----')\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare LDA model and LSI model to predict similarity.\n",
    "lda_i = similarities.MatrixSimilarity(lda[corpra])\n",
    "m = D.doc2bow(cleandata(test_data))\n",
    "# perform some queries\n",
    "similar_lda = lda_i[lda[m]]\n",
    "# Sort the similarities\n",
    "LDA = sorted(enumerate(similar_lda), key=lambda item: -item[1])\n",
    "# Top 10 most similar documents:\n",
    "print(LDA[:10])\n",
    "# the most similar document\n",
    "doc_id, similarity = LDA[1]\n",
    "print(train_data[doc_id][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same similarity queries by using LSI model\n",
    "lsi_i = similarities.MatrixSimilarity(lsi[corpra])\n",
    "similar_lsi = lsi_i[lsi[m]]\n",
    "LSI = sorted(enumerate(similar_lsi), key=lambda item:-item[1])\n",
    "print(LSI[:10])\n",
    "doc_id_lsi, similarity_lsi = LSI[1]\n",
    "print(train_data[doc_id][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14981, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the correct columns and remove null values\n",
    "train = train[['Description','AdoptionSpeed']]\n",
    "train_null = np.array(train[train['Description'].isnull() == True].index)\n",
    "train = train.drop(train_null)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Descrtion based on AdoptionSpeed, so need to seperate train into 5 groups\n",
    "train0 = np.array(train[train['AdoptionSpeed'] == 0].index)\n",
    "train1 = np.array(train[train['AdoptionSpeed'] == 1].index)\n",
    "train2 = np.array(train[train['AdoptionSpeed'] == 2].index)\n",
    "train3 = np.array(train[train['AdoptionSpeed'] == 3].index)\n",
    "train4 = np.array(train[train['AdoptionSpeed'] == 4].index)\n",
    "\n",
    "adoption1 = [train1[i] for i in range(len(train0))]\n",
    "adoption2 = [train2[i] for i in range(len(train0))]\n",
    "adoption3 = [train3[i] for i in range(len(train0))]\n",
    "adoption4 = [train4[i] for i in range(len(train0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the X and Y dataset according to different AdoptionSpeeds\n",
    "X = pd.concat([train['Description'].reindex(train0), \n",
    "               train['Description'].reindex(adoption1),\n",
    "               train['Description'].reindex(adoption2), \n",
    "               train['Description'].reindex(adoption3),\n",
    "               train['Description'].reindex(adoption4)])\n",
    "\n",
    "Y = pd.concat([train['AdoptionSpeed'].reindex(train0), \n",
    "               train['AdoptionSpeed'].reindex(adoption1),\n",
    "               train['AdoptionSpeed'].reindex(adoption2), \n",
    "               train['AdoptionSpeed'].reindex(adoption3),\n",
    "               train['AdoptionSpeed'].reindex(adoption4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1640,), (1640,), (410,), (410,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the X and Y data into train and valid:\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=.2, random_state=42)\n",
    "X_train.shape, Y_train.shape, X_valid.shape, Y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data cleaning process(tokenize, lower cases, lemmatizer) for random forest\n",
    "# and apply data cleaning on X_train and X_valid\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer() # same as LDA and LSI but do not remove stop words\n",
    "\n",
    "def cleandata_rf(reviews):\n",
    "    token = [word_tokenize(i) for i in reviews]\n",
    "    token1 = [[d.lower() for d in words if d.isalpha() == True] for words in token]\n",
    "    lemma = [[wordnet_lemmatizer.lemmatize(word) for word in doc] for doc in token1]\n",
    "    review = [\" \".join(i) for i in lemma]\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do data cleaning\n",
    "X_train_clean = cleandata_rf(X_train)\n",
    "X_valid_clean = cleandata_rf(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning model(randomforest) on Description\n",
    "tfvec = tfid(stop_words='english', ngram_range=(1, 1), lowercase=False)\n",
    "mb = multi()\n",
    "randomforest = randomf()\n",
    "pipe = Pipeline([('vectorizer', tfvec),('rf', mb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.44      0.31      0.36        87\n",
      "          1       0.32      0.24      0.27        79\n",
      "          2       0.13      0.27      0.17        66\n",
      "          3       0.27      0.24      0.26        94\n",
      "          4       0.30      0.21      0.25        84\n",
      "\n",
      "avg / total       0.30      0.26      0.27       410\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elainny/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# Y_pred and report\n",
    "pipe.fit(X_train_clean, Y_train)\n",
    "Y_pred = pipe.predict(X_valid_clean)\n",
    "print(report(Y_valid, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
